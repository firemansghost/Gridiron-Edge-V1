# Phase 2.5: Ridge Regularization for Quadratic Calibration

## Overview

Phase 2.5 enhances the quadratic calibration model with **L2 ridge regularization** to prevent overfitting and improve generalization to unseen games.

## Problem Statement

The unregularized quadratic model:
```
spread = Î± + Î²â‚Ã—RD + Î²â‚‚Ã—RDÂ² + Î²â‚ƒÃ—talent_z + Î²â‚„Ã—HFA + class_dummies
```

Can suffer from:
1. **Overfitting**: Coefficients become too large, fitting noise in training data
2. **High variance**: Small changes in training data cause large changes in predictions
3. **Poor generalization**: Model performs well on training games but poorly on new games
4. **Correlated features**: When features are correlated (e.g., rating_diff and talent_diff), coefficients become unstable

## Ridge Regularization (L2)

### Formula

Ridge regression adds a penalty term to the loss function:

```
Loss = Î£(y - Å·)Â² + Î» Î£Î²Â²
       â†‘            â†‘
   fit to data   penalty for large coefficients
```

Where:
- `Î»` (lambda) is the regularization strength
- Higher `Î»` = more shrinkage toward zero
- `Î» = 0` = unregularized model

### How It Works

1. **Shrinks coefficients**: Large coefficients are penalized, pushing them toward zero
2. **Reduces variance**: Smaller coefficients are less sensitive to noise
3. **Handles multicollinearity**: When features are correlated, ridge distributes weight among them
4. **Never eliminates features**: Unlike Lasso (L1), ridge shrinks but doesn't zero out coefficients

### Benefits for Gridiron Edge

1. **Better predictions for extreme matchups**:
   - Prevents model from over-relying on quadratic term for blowouts
   - Shrinks talent gap coefficient when it's not consistently predictive

2. **Stable coefficients across weeks**:
   - Model coefficients won't swing wildly as new game data arrives
   - More consistent betting recommendations week-to-week

3. **Improved generalization**:
   - Better performance on playoff games (out-of-sample)
   - More accurate predictions for unusual matchups

4. **Handles feature correlation**:
   - Rating diff and talent diff are often correlated (good teams have better recruits)
   - Ridge prevents unstable coefficient estimates

## Implementation

### Feature Matrix (X)

```
X = [
  1,                    // X0: intercept (not penalized)
  rating_diff,          // X1: linear term
  rating_diffÂ²,         // X2: quadratic term  
  talent_diff_z,        // X3: talent gap (z-score)
  is_P5_G5,             // X4: P5 vs G5 dummy
  is_P5_FCS,            // X5: P5 vs FCS dummy
  is_G5_G5,             // X6: G5 vs G5 dummy
  is_G5_FCS,            // X7: G5 vs FCS dummy
  hfa_team_home         // X8: team-specific HFA
]
```

### Gradient Descent with L2 Penalty

```typescript
for each iteration:
  for each coefficient Î²_j:
    // Gradient of squared error
    grad = Î£(error Ã— X_j)
    
    // Add L2 penalty gradient (skip intercept)
    if (j > 0):
      grad += Î» Ã— Î²_j
    
    // Update coefficient
    Î²_j -= learning_rate Ã— grad / n
```

### Cross-Validation for Î» Selection

Uses 5-fold cross-validation to find optimal Î»:

```
Test Î» âˆˆ [0, 0.01, 0.05, 0.1, 0.5, 1.0, 2.0, 5.0]

For each Î»:
  For each fold:
    Train on 4 folds
    Validate on 1 fold
    Record RMSE
  
  Average RMSE across folds

Choose Î» with lowest average RMSE
```

## Usage

### Basic Usage

```bash
# Auto-select Î» via cross-validation
npm run calibrate:ridge 2025 1-12

# Specify Î» manually
npm run calibrate:ridge 2025 1-12 0.1
```

### Output

```
ğŸ“Š PHASE 2.5: RIDGE REGULARIZED QUADRATIC CALIBRATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

   Season: 2025
   Weeks: 1-12
   Model: Quadratic + Talent + Class + Team HFA + Ridge L2

ğŸ” Cross-validation with 5 folds...
   Î»=0.000: RMSE=8.245, RÂ²=0.3821
   Î»=0.010: RMSE=8.198, RÂ²=0.3854
   Î»=0.050: RMSE=8.142, RÂ²=0.3901
   Î»=0.100: RMSE=8.115, RÂ²=0.3925  â† Best
   Î»=0.500: RMSE=8.287, RÂ²=0.3788
   Î»=1.000: RMSE=8.512, RÂ²=0.3621
   Î»=2.000: RMSE=8.934, RÂ²=0.3245
   Î»=5.000: RMSE=9.687, RÂ²=0.2512

   âœ… Best Î»: 0.100 (RMSE: 8.115)

ğŸ“Š RIDGE REGRESSION RESULTS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“‹ HYPERPARAMETER:
   Î» (regularization): 0.1000

ğŸ“‹ COEFFICIENTS:
   Î±  (intercept):         0.3421
   Î²â‚ (rating_diff):       6.2341
   Î²â‚‚ (rating_diffÂ²):      0.5128
   Î²â‚ƒ (talent_diff_z):     1.2456
   Î²â‚„ (P5_G5 dummy):      -2.1234
   Î²â‚… (P5_FCS dummy):     -8.4567
   Î²â‚† (G5_G5 dummy):      -1.5432
   Î²â‚‡ (G5_FCS dummy):     -5.6789
   Î²â‚ˆ (hfa_team_home):     0.9876

ğŸ“ˆ FIT QUALITY:
   RÂ²:          0.3925 (39.3%)
   Adjusted RÂ²: 0.3876 (38.8%)
   RMSE:        8.12 points
   âœ… Good fit

ğŸ¯ REGULARIZATION EFFECT:
   Unregularized RÂ²:  0.3950
   Regularized RÂ²:    0.3925
   Unregularized RMSE: 8.18 pts
   Regularized RMSE:   8.12 pts
   âœ… Ridge improves generalization
```

## Interpretation

### Coefficient Shrinkage

| Feature | Unregularized | Regularized (Î»=0.1) | Shrinkage |
|---------|---------------|---------------------|-----------|
| Î²â‚ (linear) | 6.45 | 6.23 | -3.4% |
| Î²â‚‚ (quadratic) | 0.58 | 0.51 | -12.1% |
| Î²â‚ƒ (talent_z) | 1.47 | 1.25 | -15.0% |
| Î²â‚ˆ (HFA) | 1.12 | 0.99 | -11.6% |

**Observations**:
- Linear term (Î²â‚) shrinks least â†’ most predictive, most stable
- Quadratic term (Î²â‚‚) shrinks more â†’ helps with extreme matchups but prone to overfitting
- Talent gap (Î²â‚ƒ) shrinks most â†’ useful signal but noisy, benefits from regularization

### Performance Metrics

- **RÂ² decrease**: Small drop (0.395 â†’ 0.393) is acceptable tradeoff
- **RMSE improvement**: Lower validation RMSE indicates better generalization
- **Adjusted RÂ²**: Accounts for number of features, more honest measure

### When Ridge Helps Most

1. **Limited training data**: < 100 games per season
2. **Correlated features**: Rating and talent are often correlated
3. **Extreme predictions**: Prevents quadratic term from exploding
4. **Out-of-sample testing**: Playoff games, new season starts

## Comparison to Alternatives

### Unregularized (Î» = 0)
- âœ… Best fit to training data
- âŒ Overfits, poor generalization
- âŒ Unstable coefficients

### Ridge (L2)
- âœ… Better generalization
- âœ… Stable coefficients
- âœ… Handles correlated features
- âŒ Doesn't eliminate features
- âœ… **Recommended for production**

### Lasso (L1)
- âœ… Feature selection (zeros out coefficients)
- âŒ Can arbitrarily pick one of correlated features
- âŒ Unstable feature selection
- âŒ Not implemented (yet)

### Elastic Net (L1 + L2)
- âœ… Combines benefits of both
- âœ… Feature selection + stability
- âŒ More complex, requires tuning two hyperparameters
- âŒ Not implemented (yet)

## Integration with Gridiron Edge

### Current State (Phase 2.4)

```typescript
// Unregularized quadratic model
const modelSpread = alpha 
  + beta1 * ratingDiff 
  + beta2 * ratingDiff * ratingDiff 
  + HFA;
```

### Phase 2.5 Update

```typescript
// Ridge-regularized model (9 features)
const modelSpread = coef[0]
  + coef[1] * ratingDiff
  + coef[2] * ratingDiff * ratingDiff
  + coef[3] * talentDiffZ
  + coef[4] * isP5_G5
  + coef[5] * isP5_FCS
  + coef[6] * isG5_G5
  + coef[7] * isG5_FCS
  + coef[8] * hfaTeamHome;
```

### Deployment

1. **Run calibration weekly**: After Week 12, recompute coefficients with full season data
2. **Store coefficients**: Save to `modelConfig` or database
3. **Update API**: Use ridge coefficients in spread calculation
4. **Monitor performance**: Track RMSE on new games vs. unregularized baseline

## Advanced Topics

### Î» Selection Strategies

1. **Cross-validation** (implemented):
   - Most common, reliable
   - Tests Î» on held-out data
   - Chooses Î» with lowest validation error

2. **Information criteria** (future):
   - AIC, BIC balance fit and complexity
   - Faster than cross-validation
   - Theoretical justification

3. **Bayesian approach** (future):
   - Treat Î» as random variable with prior
   - Get posterior distribution over coefficients
   - Naturally quantifies uncertainty

### Coefficient Interpretation with Ridge

âš ï¸ **Important**: Ridge coefficients are **not directly interpretable** as causal effects!

- Coefficients are shrunk toward zero
- Magnitude depends on Î» choice
- Correlated features share weight
- **Use coefficients for prediction, not explanation**

For interpretation, use:
- **Permutation importance**: Shuffle feature, measure RMSE increase
- **Partial dependence plots**: Hold others constant, vary one feature
- **SHAP values**: Game-theoretic attribution of prediction to features

## Testing & Validation

### Unit Tests

```typescript
// Test 1: Ridge converges
expect(ridgeRegression(X, y, 0.1).rmse).toBeLessThan(10);

// Test 2: Higher Î» shrinks coefficients
const coef0 = ridgeRegression(X, y, 0);
const coef1 = ridgeRegression(X, y, 1);
expect(Math.abs(coef1[1])).toBeLessThan(Math.abs(coef0[1]));

// Test 3: Cross-validation selects reasonable Î»
const { bestLambda } = crossValidateRidge(X, y, lambdas);
expect(bestLambda).toBeGreaterThan(0);
expect(bestLambda).toBeLessThan(5);
```

### Integration Tests

```bash
# Test on historical data (Weeks 1-10)
npm run calibrate:ridge 2025 1-10 0.1

# Validate on held-out data (Weeks 11-12)
# Compare predictions vs. actual spreads
# Expect RMSE < unregularized model
```

### Production Monitoring

Track these metrics weekly:
- **Prediction RMSE**: On new games
- **Coefficient stability**: Week-to-week changes
- **Feature importance**: Which features matter most
- **Calibration**: Are predicted spreads well-calibrated?

## Next Steps (Future Phases)

### Phase 2.6: Elastic Net
- Combine L1 + L2 regularization
- Automatic feature selection
- Better for high-dimensional feature spaces

### Phase 2.7: Bayesian Ridge
- Probabilistic interpretation
- Confidence intervals on predictions
- Automatic Î» selection via posterior

### Phase 2.8: Non-linear Models
- Neural networks with L2 weight decay
- Gradient boosting with regularization
- Kernel ridge regression

### Phase 2.9: Online Learning
- Update coefficients as new games arrive
- Stochastic gradient descent
- Adaptive Î» based on recent performance

## References

- **Tibshirani (1996)**: Regression Shrinkage and Selection via the Lasso
- **Hastie et al. (2009)**: The Elements of Statistical Learning
- **James et al. (2013)**: An Introduction to Statistical Learning
- **Murphy (2022)**: Probabilistic Machine Learning: An Introduction

## Summary

âœ… **Ridge regularization (L2) benefits**:
1. Prevents overfitting
2. Improves generalization to new games
3. Stabilizes coefficients
4. Handles correlated features
5. Simple to implement and tune

âœ… **Recommended for production**: Î» â‰ˆ 0.05-0.2 based on cross-validation

âœ… **Run weekly**: Refit after each week to incorporate new data

âœ… **Monitor performance**: Track RMSE on new games vs. baseline

